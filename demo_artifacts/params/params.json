{
  "model_architecture": "transformer_agi",
  "parameters": 175000000000,
  "layers": 96,
  "attention_heads": 96,
  "hidden_size": 12288,
  "context_length": 8192,
  "training_data_size": "500TB",
  "compute_requirements": "8x A100 GPUs minimum"
}